{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM+UCPRsDTo2+q60hSb/7cf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/n-bzy/iannwtf/blob/main/homework10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Disclaimer: Code is not working, i. e. model cannot train on data\n",
        "\n",
        "Preprocessing (lower case everything, remove special characters and split into list of strings) and creating word embeddings (initialize vocabulary and exchange words for numbers) for input-target-pairs work as far as it is running and creating the desired output.\n",
        "\n",
        "Initializing of the model works but not training as it states \"true_classes must be a matrix [Op:LogUniformCandidateSampler]\" which we guess has to do with the `tf.nn.nce_loss function()` . We tried different inputs, tried to exchange the loss function with `tf.nn.sigmoid_cross_entropy_with_logits()` and also `tf.keras.losses.CategoricalCrossentropy()` and tried to understand how the word2vec tutorial implements the model but none of it worked. \n",
        "\n",
        "A last try would be to include negative sampling but since we could not really understand what `tf.random.log_uniform_candidate_sampler()` does or how to use it correctly and time is running out after several hours spent on this homework we can not finish it properly."
      ],
      "metadata": {
        "id": "AZuw8WGyJtxo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow_text"
      ],
      "metadata": {
        "id": "ON2SBW7_6hBn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kJHoPD_YEfjo",
        "outputId": "92c21d49-7ebb-454f-e70c-8a3cb9488c34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# bash code to mount the drive\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount (\"/content/drive\")\n",
        "os.chdir(\"/content/drive/MyDrive\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "#import tensorflow_text as tf_text\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "file_path = f\"/content/drive/MyDrive/bible.txt\"\n",
        "\n",
        "with open(file_path, \"r\") as f:\n",
        "    text = f.read()\n",
        "\n",
        "print(text[:100])"
      ],
      "metadata": {
        "id": "q4wWfHfqGSW6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7555eff-1bcf-4ba6-eadb-f3f387b92677"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The First Book of Moses:  Called Genesis\n",
            "\n",
            "\n",
            "1:1 In the beginning God created the heaven and the earth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocess"
      ],
      "metadata": {
        "id": "mlWiWQ9U9MRs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_voc(text,size):\n",
        "    \"\"\"Preprocess text data by lower case, remove special characters and split \n",
        "    words as well as sort from most common down\n",
        "    input: text,vocabulary size\n",
        "    output: preprocessed text, sorted vocabulary\"\"\"\n",
        "\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"[^a-z]+\", \" \", text)\n",
        "    text = tf.strings.split(text) \n",
        "\n",
        "    text_n = text.numpy().tolist()\n",
        "    text_n.sort()\n",
        "    counts = { text_n[0] : 1 }\n",
        "    current_word = text_n[0]\n",
        "    for i in text_n[1:]: \n",
        "        if i == current_word:\n",
        "            counts[current_word] += 1\n",
        "        else:\n",
        "            current_word = i\n",
        "            counts.update({current_word:1})\n",
        "    counts = {key: val for key, val in sorted(counts.items(), key = lambda ele: ele[1], reverse = True)}\n",
        "    voc = tf.convert_to_tensor(list(counts.keys())[:size])\n",
        "    return text, voc\n",
        "\n",
        "tex, voc = preprocess_voc(text,size=10000)\n",
        "print(voc, tex[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cogh0cJfferw",
        "outputId": "05a5c197-b1be-487b-9747-ac53d7c02233"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([b'the' b'and' b'of' ... b'forbiddeth' b'forborn' b'forcible'], shape=(10000,), dtype=string) tf.Tensor(\n",
            "[b'the' b'first' b'book' b'of' b'moses' b'called' b'genesis' b'in' b'the'\n",
            " b'beginning'], shape=(10,), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def words_to_number(text, voc):\n",
        "    \"\"\"Exchanges the words in the text with numbers by using the vocabulary.\n",
        "    All words not included in the vocabulary are exchanged by 10000 aka. [UNK]\n",
        "    input: preprocessed text, vocabulary\n",
        "    output: text as list of numbers\"\"\"\n",
        "    text = text.numpy().tolist()\n",
        "    voc = voc.numpy().tolist()\n",
        "    for item in range(len(text)):\n",
        "        if text[item] in voc:\n",
        "            text[item] = voc.index(text[item])\n",
        "        else: \n",
        "            # [UNK] = index 10000\n",
        "            text[item] = 10000\n",
        "    return text\n",
        "\n",
        "tex_n = words_to_number(tex,voc)\n",
        "print(len(tex_n), tex_n[:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IuYh9W_Zwv-h",
        "outputId": "9c48539d-5fcc-4318-92e3-a8aaeb74b631"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "791829 [0, 216, 401, 2, 132, 160, 10000, 5, 0, 680, 26, 1297, 0, 170, 1, 0, 111, 1, 0, 111]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Input-Target-Pairs"
      ],
      "metadata": {
        "id": "6yn03T8L3rLC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def input_target_pairs(text):\n",
        "    \"\"\"Create input-target-pairs by shifting the dataset for 1 and 2 items and\n",
        "    zipping them together\n",
        "    input: text as list of numbers\n",
        "    output: dataset of input-target-pairs\"\"\"\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(text)\n",
        "\n",
        "    iterator = iter(dataset) \n",
        "    iterator.get_next()\n",
        "    # same dataset just shifted for 1 item to pair with original dataset\n",
        "    shift1 = dataset.map(lambda x: iterator.get_next())\n",
        "\n",
        "    iterator2 = iter(dataset) \n",
        "    iterator2.get_next()\n",
        "    iterator2.get_next()\n",
        "    # same dataset just shifted for 2 items to pair with original dataset\n",
        "    shift2 = dataset.map(lambda x: iterator2.get_next())\n",
        "\n",
        "    # Create input-target-pairs\n",
        "    shift1up = tf.data.Dataset.zip((dataset, shift1))\n",
        "    shift2up = tf.data.Dataset.zip((dataset, shift2))\n",
        "    shift1down = tf.data.Dataset.zip((shift1, dataset))\n",
        "    shift2down = tf.data.Dataset.zip((shift2, dataset))\n",
        "\n",
        "    dataset = shift2down.concatenate(shift1up).concatenate(shift2up).concatenate(shift1down)\n",
        "    dataset = dataset.shuffle(10000).batch(64).prefetch(tf.data.AUTOTUNE)\n",
        "    return dataset\n",
        "\n",
        "\n",
        "train_ds = input_target_pairs(tex_n[:math.ceil(len(tex_n)*0.75)])\n",
        "test_ds = input_target_pairs(tex_n[math.ceil(len(tex_n)*0.75):])\n",
        "for x,t in train_ds.take(1):\n",
        "    tf.print(x.shape, t.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nqI1X-LS3u2u",
        "outputId": "b9b510f8-9a13-45d7-8e4a-2d928017d29d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorShape([64]) TensorShape([64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Class"
      ],
      "metadata": {
        "id": "aUQnJDx7_SkO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SkipGram(tf.keras.layers.Layer):\n",
        "    \"\"\"Skipgram model\"\"\"\n",
        "    def __init__(self, voc_size, emb_size):\n",
        "        \"\"\"Initialize the model, optimizer and metrics\"\"\"\n",
        "        super().__init__()\n",
        "    \n",
        "        self.voc_size = voc_size\n",
        "        self.emb_size = emb_size\n",
        "\n",
        "        self.opt = tf.keras.optimizers.Adam()\n",
        "\n",
        "        self.metrics_list = [tf.keras.metrics.Mean(name=\"loss\")]\n",
        "    \n",
        "    def build(self):\n",
        "        \"\"\"Initialize the weights and bias for score and embedding matrices\"\"\"\n",
        "        self.score = self.add_weight(shape=(self.voc_size, self.emb_size),\n",
        "                                     initializer='random_normal',\n",
        "                                     trainable=True)\n",
        "        self.score_bias = self.add_weight(shape=(self.emb_size,),\n",
        "                                   initializer='random_normal',\n",
        "                                   trainable=True)\n",
        "        self.emb = self.add_weight(shape=(self.voc_size, self.emb_size),\n",
        "                                   initializer='random_normal',\n",
        "                                   trainable=True)\n",
        "\n",
        "    def __call__(self, x, training=False):\n",
        "        \"\"\"Create the embedding lookup with the embbeding matrix and data input\"\"\"\n",
        "        emb = tf.nn.embedding_lookup(self.emb, x)\n",
        "        return emb\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return self.metrics_list\n",
        "    \n",
        "    def reset_metrics(self):\n",
        "        for metric in self.metrics:\n",
        "            metric.reset_state()   \n",
        "\n",
        "    def train(self, data):\n",
        "        input, target = data\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            y = self(input, training=True)\n",
        "            \n",
        "            # Not working\n",
        "            loss = tf.nn.nce_loss(weights=self.score, biases=self.score_bias, labels=target, inputs=y, \n",
        "                                  num_sampled=2, num_classes=self.voc_size)\n",
        "            loss = tf.reduce_mean(loss)\n",
        "            \n",
        "        gradients = tape.gradient(loss, self.trainable_variables)\n",
        "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
        "        \n",
        "        self.metrics[0].update_state(loss)\n",
        "        \n",
        "        return {m.name : m.result() for m in self.metrics} \n",
        "\n",
        "    def test(self,data): \n",
        "        input, target = data\n",
        "\n",
        "        y = self(input, training=False)\n",
        "        loss = tf.nn.nce_loss(weights=self.score, labels=target, inputs=y, \n",
        "                              num_sampled=2, num_classes=self.voc_size)\n",
        "        loss = tf.reduce_mean(loss)\n",
        "        \n",
        "        self.metrics[0].update_state(loss)\n",
        "        \n",
        "        return {m.name : m.result() for m in self.metrics}   "
      ],
      "metadata": {
        "id": "UKEGJ_hn_Thx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_skip = SkipGram(10001, 64)\n",
        "model_skip.build()"
      ],
      "metadata": {
        "id": "SY7RuVZNOdOu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tqdm\n",
        "\n",
        "def training_loop(model, train, val, epochs):\n",
        "    \"\"\"Train and test the SkipGram for given epochs on given text data\"\"\"\n",
        "\n",
        "    # Save loss in a list for visualization\n",
        "    lists = []\n",
        "\n",
        "    for n in range(epochs):\n",
        "        print(f\"Epoch {n}:\")\n",
        "        \n",
        "        for data in tqdm.tqdm(train, position=0, leave=True):\n",
        "            metrics = model.train(data)\n",
        "\n",
        "        # Add metrics to list\n",
        "        lists.append(metrics)\n",
        "        print([f\"{key}: {value.numpy()}\" for (key,value) in metrics.items()])\n",
        "        model.reset_metrics()\n",
        "\n",
        "        for data in tqdm.tqdm(val, position=0, leave=True):\n",
        "            metrics = model.test(data)\n",
        "\n",
        "        # Add metrics to list\n",
        "        lists.append(metrics)\n",
        "        print([f\"{key}: {value.numpy()}\" for (key,value) in metrics.items()])\n",
        "        model.reset_metrics()\n",
        "        \n",
        "    return lists\n",
        "\n",
        "li = training_loop(model_skip, train_ds, test_ds, epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 783
        },
        "id": "xT-AmUgwO6o2",
        "outputId": "aa2b4e45-95c7-4f71-f5e3-e928bbb294bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/2375488 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "() ()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-53051216a3d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mli\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_skip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-19-53051216a3d5>\u001b[0m in \u001b[0;36mtraining_loop\u001b[0;34m(model, train, val, epochs)\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m# Add metrics to list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-f59113c1ec90>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             loss = tf.nn.nce_loss(weights=self.score, biases=self.score_bias, labels=target, inputs=y, \n\u001b[0m\u001b[1;32m     43\u001b[0m                                   num_sampled=2, num_classes=self.voc_size)\n\u001b[1;32m     44\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   7162\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7163\u001b[0m   \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7164\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: true_classes must be a matrix [Op:LogUniformCandidateSampler]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7og-UjKyHzXP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}