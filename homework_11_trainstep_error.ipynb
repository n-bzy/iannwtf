{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPF5DRQeeeFxFiTLeAjuoB9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/n-bzy/iannwtf/blob/main/homework_11_trainstep_error.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece\n",
        "!pip install tensorflow_text"
      ],
      "metadata": {
        "id": "pDpMoNrSimPV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67304425-ae09-4c08-f6bc-0211cac5aa2e"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.8/dist-packages (0.1.97)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow_text in /usr/local/lib/python3.8/dist-packages (2.11.0)\n",
            "Requirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow_text) (0.12.0)\n",
            "Requirement already satisfied: tensorflow<2.12,>=2.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow_text) (2.11.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (1.6.3)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (1.14.1)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (0.4.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (3.19.6)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (1.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (4.4.0)\n",
            "Requirement already satisfied: tensorboard<2.12,>=2.11 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (2.11.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (57.4.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (3.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (1.15.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (23.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (2.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (1.51.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (2.11.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (23.1.21)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (15.0.6.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (3.3.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (0.2.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (1.21.6)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (0.30.0)\n",
            "Requirement already satisfied: keras<2.12,>=2.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (2.11.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.8/dist-packages (from astunparse>=1.6.0->tensorflow<2.12,>=2.11.0->tensorflow_text) (0.38.4)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (2.16.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (3.4.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (2.25.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (0.6.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (5.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (6.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (1.24.3)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (4.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (3.12.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_text as tf_text\n",
        "from tensorflow.keras.layers import Layer\n",
        "from tensorflow.keras import Model\n",
        "import sentencepiece as sp\n",
        "import math\n",
        "from google.colab import drive\n",
        "import os\n",
        "import io\n",
        "import re\n",
        "import datetime\n",
        "import tqdm"
      ],
      "metadata": {
        "id": "99OY2-OLfxLP"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#bash code to mount the drive\n",
        "drive.mount(\"/content/drive\")\n",
        "os.chdir(\"drive/MyDrive\")"
      ],
      "metadata": {
        "id": "XwqdnehSga-f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "outputId": "de1419d3-6798-4371-8f98-07646e2255f8"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-7479736ec316>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#bash code to mount the drive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"drive/MyDrive\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'drive/MyDrive'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#load the text file to which the model is fitted as a string\n",
        "with open(\"bible.txt\", \"r\") as f:\n",
        "  text = f.read()"
      ],
      "metadata": {
        "id": "U_EMn2r2gMci"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZjv2v8EP4jo",
        "outputId": "0edcc8dd-7a4e-490b-adbf-4be6f1b6c93b"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The First Book of Moses:  Called Genesis\n",
            "\n",
            "\n",
            "1:1 In the beginning God created the heaven and the earth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameters"
      ],
      "metadata": {
        "id": "o_CrIxlMX3-t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "WINDOW_SIZE = 128 #sequence length, try between 32-256\n",
        "VOCAB_SIZE = 2000 #try between 2000-7000\n",
        "EMBEDDING_DIM = 64 #try between 64-256\n",
        "BATCH_SIZE = 128"
      ],
      "metadata": {
        "id": "95NE7uL9X54M"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing\n"
      ],
      "metadata": {
        "id": "bfBXcSdAfsa_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "wdYkR6qCfpR9"
      },
      "outputs": [],
      "source": [
        "def tokenize(text):\n",
        "    #data cleaning: convert to lower case + remove all characters that aren't letters or spaces\n",
        "    text = re.sub(\"[^a-z]+\",\" \",text.lower()) \n",
        "    #train sentence-piece tokenizer on text data\n",
        "    sp.SentencePieceTrainer.train(input=\"bible.txt\", model_prefix='tokenizer_model', model_type=\"unigram\", vocab_size=VOCAB_SIZE)\n",
        "    #deserialize the trained model file to load it in the correct format\n",
        "    trained_tokenizer_model = tf.io.gfile.GFile('tokenizer_model.model', \"rb\").read()\n",
        "    #load the model as a tokenizer that can be used inside a tensorflow model\n",
        "    tokenizer = tf_text.SentencepieceTokenizer(\n",
        "        model=trained_tokenizer_model, out_type=tf.int32, nbest_size=-1, alpha=1, reverse=False,\n",
        "        add_bos=False, add_eos=False, return_nbest=False, name=None)\n",
        "    #tokenize text data with trained Sentence Piece tokenizer\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    return tokens, tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens, tokenizer = tokenize(text)"
      ],
      "metadata": {
        "id": "3rYBWTyF13fe"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Flyl1_rX7Iet",
        "outputId": "51d5b0c1-6e25-477d-d230-08f038d566e5"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([  4 345 739 ...  35 196 122], shape=(1056932,), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(tokens):\n",
        "    \"\"\"Data preprocessing on the tokenized text\"\"\"\n",
        "\n",
        "    # create sliding window inputs of length m+1 \n",
        "    data = tf_text.sliding_window(data=tokens, width=WINDOW_SIZE+1)\n",
        "\n",
        "    \"\"\"\n",
        "    #create input seq and target seq for each token window\n",
        "    inputs = []\n",
        "    targets = []\n",
        "    for n in range(len(data)):\n",
        "        inputs.append(data[n][:WINDOW_SIZE])\n",
        "        targets.append(data[n][-WINDOW_SIZE:])\n",
        "    \"\"\"\n",
        "\n",
        "    #create a tensorflow dataset\n",
        "    data = tf.data.Dataset.from_tensor_slices(data)\n",
        "    # cache the dataset\n",
        "    data = data.cache()\n",
        "    #shuffle, batch, prefetch\n",
        "    data = data.shuffle(1000).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "    \n",
        "    return data"
      ],
      "metadata": {
        "id": "mZj-QRcnI_T9"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create train and validation data sets\n",
        "train_ds = preprocess(tokens[:math.ceil(len(tokens)*0.9)])\n",
        "val_ds = preprocess(tokens[math.ceil(len(tokens)*0.9):])\n",
        "\n",
        "for x in train_ds.take(1):\n",
        "  print(x.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0c6GHYCz-RQ",
        "outputId": "0a67b858-840e-4c9e-f2ba-63b38ee7e262"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(128, 129)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The model"
      ],
      "metadata": {
        "id": "lJCSxJozWSer"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EmbeddingBlock(Layer):\n",
        "    \"\"\"Embedding layer that embeds the individual token indices + their position in the input\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"Constructor contains 2 embedding layers, one for token indices, one for token positions\"\"\"\n",
        "        super().__init__()\n",
        "        self.idx_embedding = tf.keras.layers.Embedding(input_dim = VOCAB_SIZE, output_dim = EMBEDDING_DIM)\n",
        "        self.pos_embedding = tf.keras.layers.Embedding(input_dim = WINDOW_SIZE, output_dim = EMBEDDING_DIM)\n",
        "        \n",
        "\n",
        "    def call(self, input):\n",
        "        \"\"\"Forward step\"\"\"\n",
        "        #construct a tensor, where its elements are used as indices to look up the positional code for each sub-word\n",
        "        tensor = tf.range(0, len(input))\n",
        "        tensor = tf.expand_dims(tensor,-1)\n",
        "        #feed the token index embedding layer with the input sequence\n",
        "        e1 = self.idx_embedding(input)\n",
        "        #feed the positional embedding layer with the  range tensor\n",
        "        e2 = self.pos_embedding(tensor)\n",
        "        #add the two embeddings\n",
        "        sum = e1 + e2\n",
        "        return sum"
      ],
      "metadata": {
        "id": "x-NGtsT9WS92"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(Layer):\n",
        "    \"\"\" \"\"\"\n",
        "    def __init__(self):\n",
        "        \"\"\"Contructor that works with 2-4 attention heads\"\"\"\n",
        "        super().__init__()\n",
        "        self.mha = tf.keras.layers.MultiHeadAttention(num_heads = 3, key_dim = EMBEDDING_DIM)\n",
        "        self.dense1 = tf.keras.layers.Dense(units = 128, activation = tf.nn.relu)\n",
        "        self.dense2 = tf.keras.layers.Dense(units = EMBEDDING_DIM)\n",
        "        self.dropOut1 = tf.keras.layers.Dropout(rate = 0.1)\n",
        "        self.dropOut2 = tf.keras.layers.Dropout(rate = 0.1)\n",
        "        self.norm1 = tf.keras.layers.LayerNormalization(epsilon = 0.000001)\n",
        "        self.norm2 = tf.keras.layers.LayerNormalization(epsilon = 0.000001)\n",
        "\n",
        "    def call(self, input, training=False):\n",
        "        \"\"\"Forward step\"\"\"\n",
        "        #give input to MHA-layer as both value and query arguments\n",
        "        #causal mask is True such that model does not attend to future tokens\n",
        "        mha_out = self.mha(query=input, value=input, use_causal_mask=True) \n",
        "        #use dropout on the output of MHA layer \n",
        "        drop_out = self.dropOut1(mha_out,training=training)\n",
        "        #add result to layer input\n",
        "        drop_out += input\n",
        "        #apply layer normalization\n",
        "        ln_out = self.norm1(drop_out)\n",
        "\n",
        "        #use normalized output for another residual connection\n",
        "        x = self.dense1(ln_out)\n",
        "        x = self.dense2(x)\n",
        "        x = self.dropOut2(x,training=training)\n",
        "        x += ln_out\n",
        "        x = self.norm2(x)       \n",
        "\n",
        "        return x\n",
        "     "
      ],
      "metadata": {
        "id": "upaRMKL2aalk"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(Model):\n",
        "  \"\"\"Model contains methods for initialization, calling, metric reset, trainstep, and text generation\"\"\"\n",
        "\n",
        "  def __init__(self, tokenizer):\n",
        "      \"\"\"Initialization method sets up all parameters that will be used by other methodsMy\n",
        "        - tokenizer: sentence piece tokenizer to output text, not just token IDs\n",
        "        - optimizer: Adam with a learning rate of 0.001\n",
        "        - loss_function: SparseCategoricalCrossentropy -> targets aren't one-hot encoded, but indices\n",
        "        - metrics: Mean Loss, Categorical Accuracy, Top K Categorical Accuracy\n",
        "      \"\"\"\n",
        "      super().__init__()\n",
        "      self.tokenizer = tokenizer \n",
        "      self.optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "      self.loss_function = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "      \n",
        "      self.metrics_list = [\n",
        "                      tf.keras.metrics.Mean(name=\"loss\"),\n",
        "                      tf.keras.metrics.CategoricalAccuracy(name=\"acc\"),\n",
        "                      tf.keras.metrics.TopKCategoricalAccuracy(3,name=\"top-3-acc\") \n",
        "                      ]\n",
        "      \n",
        "      self.layerList = [\n",
        "                        EmbeddingBlock(),\n",
        "                        TransformerBlock(),\n",
        "                        tf.keras.layers.Dense(units=VOCAB_SIZE)\n",
        "                        ]\n",
        "    \n",
        "  def call(self, x):\n",
        "      \"\"\"Forward step through all layers\"\"\"\n",
        "      for layer in self.layerList.layers:\n",
        "          x = layer(x)\n",
        "      return x\n",
        "  \n",
        "  def reset_metrics(self):\n",
        "      for metric in self.metrics:\n",
        "        metric.reset_states()\n",
        "  \n",
        "  @tf.function\n",
        "  def train_step(self, data):\n",
        "      #split data into input and target sequences\n",
        "      x = data[:,:WINDOW_SIZE]\n",
        "      targets = data[:,1:]\n",
        "      #tape loss and prediction\n",
        "      with tf.GradientTape() as tape:\n",
        "          predictions = self(x, training=True)\n",
        "          loss = self.loss_function(targets, predictions) + tf.reduce_sum(self.losses)\n",
        "      gradients = tape.gradient(loss, self.trainable_variables)\n",
        "      self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
        "      #update loss metric\n",
        "      self.metrics[0].update_state(loss)\n",
        "      #update the two accuracy metrics\n",
        "      for metric in self.metrics[1:]:\n",
        "          metric.update_state(targets,predictions)\n",
        "      #return a dictionary mapping metric names to current value\n",
        "      return {m.name: m.result() for m in self.metrics}\n",
        "\n",
        "  @tf.function\n",
        "  def test_step(self, data):\n",
        "      #split data into input and target sequences\n",
        "      x = data[:,:WINDOW_SIZE]\n",
        "      targets = data[:,1:]\n",
        "      #get loss and prediction\n",
        "      predictions = self(x, training=False)\n",
        "      loss = self.loss_function(targets, predictions) + tf.reduce_sum(self.losses)\n",
        "      #update loss metric\n",
        "      self.metrics[0].update_state(loss)\n",
        "      #update the two accuracy metrics\n",
        "      for metric in self.metrics[1:]:\n",
        "          metric.update_state(targets,predictions)\n",
        "      #return a dictionary mapping metric names to current value\n",
        "      return {m.name: m.result() for m in self.metrics}\n",
        "  \n",
        "  def generate_text(self, prompt, length, top_k=100):\n",
        "      \"\"\"Method \n",
        "        - prompt: the text (string)\n",
        "        - length: the desired output length \n",
        "        - top_k: specifies the amount of most likely (sub-)words we want to sample from\n",
        "        - returns a continuation of the input prompt of a specified length\"\"\"\n",
        "      \n",
        "      #tokenize prompt\n",
        "      prompt = self.tokenizer.tokenize(prompt)\n",
        "\n",
        "      #generate next token of current prompt until requested output length is reached\n",
        "      while len(prompt) <= length:\n",
        "        #add batch dimension\n",
        "        prompt = tf.expand_dims(prompt, axis=0)\n",
        "        #create padded prompt\n",
        "        paddings = tf.constant([[0, 0, ], [WINDOW_SIZE-len(prompt), 0 ]])\n",
        "        pad_prompt = tf.pad(prompt, paddings, mode=\"CONSTANT\", constant_values=0)\n",
        "        #obtain the logits from the model by calling it on the padded prompt\n",
        "        #logits = unnormalized scores for likelihood of each token in vocabulary to be next\n",
        "        logits = self.call(pad_prompt)\n",
        "\n",
        "        #apply top_k to find the k most likely next tokens based on their logit scores\n",
        "        top_k_logits, indices = tf.math.top_k(logits, top_k, sorted=True)\n",
        "        #indices = tf.cast(indices, tf.float32)\n",
        "        #sample next token from top_k tokens\n",
        "        next_token = tf.random.categorical(indices, num_samples=1)\n",
        "        #add new token to prompt\n",
        "        prompt = tf.concat((prompt,next_token), axis=1)\n",
        "        #truncate length of input by cutting of beginning of prompt\n",
        "        prompt = prompt[-len(prompt)-1:]\n",
        "  \n",
        "      #use tokenizer to detokenize the result\n",
        "      out = self.tokenizer.detokenize(prompt)\n",
        "\n",
        "      return out"
      ],
      "metadata": {
        "id": "Wb-VVPLQgq3A"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "5fvGy926w2ja"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "CREATE TENSORBOARD"
      ],
      "metadata": {
        "id": "6JuryxhDzL36"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#clean all the logs\n",
        "!rm -rf ./logs/"
      ],
      "metadata": {
        "id": "luyOdwBhxcDf"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load tensorboard extension\n",
        "%load_ext tensorboard\n",
        "\n",
        "# Define where to save the log\n",
        "config_name = \"Homework11\"\n",
        "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "#we create a train and validation writer\n",
        "train_log_path = f\"logs/{config_name}/{current_time}/train\"\n",
        "train_summary_writer = tf.summary.create_file_writer(train_log_path)\n",
        "val_log_path = f\"logs/{config_name}/{current_time}/val\"\n",
        "val_summary_writer = tf.summary.create_file_writer(val_log_path)"
      ],
      "metadata": {
        "id": "07fwshvqw3ve",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56cf8c69-d6bd-43a2-b1c2-2951539de1e4"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TRAINING LOOP"
      ],
      "metadata": {
        "id": "zC2ne3X-zPRD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_heads = 2 #normally 2-4\n",
        "starting_prompt = \"What is\"\n",
        "length = 30 #length of generated text\n",
        "\n",
        "#instantiate model\n",
        "model = Transformer(tokenizer=tokenizer)\n",
        "\n",
        "#run model on input once so the layers are built\n",
        "model(tf.keras.Input((129)))\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jh_h8vLe2WAd",
        "outputId": "09653975-b6b1-44e8-b61f-6e13ffea7325"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"transformer_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_block_2 (Embeddin  multiple                 136192    \n",
            " gBlock)                                                         \n",
            "                                                                 \n",
            " transformer_block_2 (Transf  multiple                 66624     \n",
            " ormerBlock)                                                     \n",
            "                                                                 \n",
            " dense_8 (Dense)             multiple                  130000    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 332,822\n",
            "Trainable params: 332,816\n",
            "Non-trainable params: 6\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#we will train 100 epochs (recommended are 100-600 epochs)\n",
        "for epoch in range(100):\n",
        "\n",
        "    #TRAINING DATASET\n",
        "    for data in train_ds:\n",
        "        metric = model.train_step(data)\n",
        "\n",
        "        with train_summary_writer.as_default():\n",
        "            #insert metrics into tensorboard log of current epoch\n",
        "            for metric in model.metrics:\n",
        "                tf.summary.scalar(metric.name, metric.result(), step=epoch)\n",
        "            #insert generated text into tensorboard log of current epoch\n",
        "            generated_text = model.generate_text(starting_prompt, length)\n",
        "            tf.summary.text(\"generated_text\", generated_text, step = epoch)\n",
        "    \n",
        "    #print generated text of each epoch\n",
        "    print(\"Epoch: \" + str(epoch))\n",
        "    print(model.generate_text(starting_prompt, length))\n",
        "    # reset all metrics (requires a reset_metrics method in the model)\n",
        "    model.reset_metrics() \n",
        "\n",
        "    \n",
        "    #VALIDATION DATASET\n",
        "    for data in val_ds:\n",
        "        metrics = model.test_step(data)\n",
        "\n",
        "        with val_summary_writer.as_default():\n",
        "              for metric in model.metrics:\n",
        "                  tf.summary.scalar(metric.name,metric.result(),step=epoch)\n",
        "\n",
        "    # reset all metrics\n",
        "    model.reset_metrics()\n",
        "    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 815
        },
        "id": "ic6RlRdpxrYH",
        "outputId": "0dda94f1-4694-4901-fd73-16c1d0db6a10"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-3fed4cf44ecc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m#TRAINING DATASET\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_ds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mmetric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtrain_summary_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/__autograph_generated_file8a4m605f.py\u001b[0m in \u001b[0;36mtf__train_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     27\u001b[0m                     \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                 \u001b[0mmetric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUndefined\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'metric'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m                 \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_stmt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloop_body\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'iterate_names'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'metric'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/__autograph_generated_file8a4m605f.py\u001b[0m in \u001b[0;36mloop_body\u001b[0;34m(itr)\u001b[0m\n\u001b[1;32m     25\u001b[0m                 \u001b[0;32mdef\u001b[0m \u001b[0mloop_body\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                     \u001b[0mmetric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                     \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m                 \u001b[0mmetric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUndefined\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'metric'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                 \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_stmt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloop_body\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'iterate_names'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'metric'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/metrics_utils.py\u001b[0m in \u001b[0;36mdecorated\u001b[0;34m(metric_obj, *args, **kwargs)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_context_for_symbolic_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0mupdate_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_state_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mupdate_op\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# update_op will be None in eager execution.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0mmetric_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupdate_op\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/metrics/base_metric.py\u001b[0m in \u001b[0;36mupdate_state_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m                     \u001b[0mobj_update_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol_status\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                 )\n\u001b[0;32m--> 140\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mag_update_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/metrics/base_metric.py\u001b[0m in \u001b[0;36mupdate_state\u001b[0;34m(self, y_true, y_pred, sample_weight)\u001b[0m\n\u001b[1;32m    689\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__internal__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_status_ctx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m         )\n\u001b[0;32m--> 691\u001b[0;31m         \u001b[0mmatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fn_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    692\u001b[0m         \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlosses_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m         sample_weight = losses_utils.apply_valid_mask(\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/metrics/metrics.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(yt, yp, k)\u001b[0m\n\u001b[1;32m    409\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"top_k_categorical_accuracy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m         super().__init__(\n\u001b[0;32m--> 411\u001b[0;31m             lambda yt, yp, k: metrics_utils.sparse_top_k_categorical_matches(\n\u001b[0m\u001b[1;32m    412\u001b[0m                 \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m             ),\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/metrics_utils.py\u001b[0m in \u001b[0;36msparse_top_k_categorical_matches\u001b[0;34m(y_true, y_pred, k)\u001b[0m\n\u001b[1;32m   1003\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1004\u001b[0m     matches = tf.cast(\n\u001b[0;32m-> 1005\u001b[0;31m         tf.math.in_top_k(\n\u001b[0m\u001b[1;32m   1006\u001b[0m             \u001b[0mpredictions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"int32\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1007\u001b[0m         ),\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"<ipython-input-14-0fcb90d47556>\", line 53, in train_step  *\n        metric.update_state(targets,predictions)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/utils/metrics_utils.py\", line 77, in decorated  **\n        update_op = update_state_fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/metrics/base_metric.py\", line 140, in update_state_fn\n        return ag_update_state(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/metrics/base_metric.py\", line 691, in update_state  **\n        matches = ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/metrics/metrics.py\", line 411, in <lambda>  **\n        lambda yt, yp, k: metrics_utils.sparse_top_k_categorical_matches(\n    File \"/usr/local/lib/python3.8/dist-packages/keras/utils/metrics_utils.py\", line 1005, in sparse_top_k_categorical_matches\n        tf.math.in_top_k(\n\n    ValueError: Dimensions must be equal, but are 16384 and 128 for '{{node in_top_k/InTopKV2}} = InTopKV2[T=DT_INT32](Reshape, Cast_5, in_top_k/InTopKV2/k)' with input shapes: [16384,2000], [128], [].\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "VISUALIZATION"
      ],
      "metadata": {
        "id": "3GrWCZ6p2sfG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir logs/"
      ],
      "metadata": {
        "id": "X6yhu3u22wM9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}