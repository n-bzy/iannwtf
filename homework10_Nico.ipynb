{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPeiabh61AlqOi7IH1oNEqj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/n-bzy/iannwtf/blob/main/homework10_Nico.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow_text"
      ],
      "metadata": {
        "id": "ON2SBW7_6hBn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kJHoPD_YEfjo",
        "outputId": "92c21d49-7ebb-454f-e70c-8a3cb9488c34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# bash code to mount the drive\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount (\"/content/drive\")\n",
        "os.chdir(\"/content/drive/MyDrive\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "#import tensorflow_text as tf_text\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "file_path = f\"/content/drive/MyDrive/bible.txt\"\n",
        "\n",
        "with open(file_path, \"r\") as f:\n",
        "    text = f.read()\n",
        "\n",
        "print(text[:100])"
      ],
      "metadata": {
        "id": "q4wWfHfqGSW6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7555eff-1bcf-4ba6-eadb-f3f387b92677"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The First Book of Moses:  Called Genesis\n",
            "\n",
            "\n",
            "1:1 In the beginning God created the heaven and the earth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocess"
      ],
      "metadata": {
        "id": "mlWiWQ9U9MRs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Option 1: Normalize, split words, sort by most common words and replace all \n",
        "# words not under common size with [UNK]\n",
        "\n",
        "def preprocess(text, size):\n",
        "    \"Preprocessing the text file for use in a NLP model\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"[^a-z]+\", \" \", text)\n",
        "    text = tf.strings.split(text)\n",
        "\n",
        "    vec_layer = tf.keras.layers.TextVectorization(max_tokens=size,\n",
        "                                                        standardize=None,\n",
        "                                                        split=None,\n",
        "                                                        output_mode='int')\n",
        "    vec_layer.adapt(text)\n",
        "    voc = vec_layer.get_vocabulary()\n",
        "    \n",
        "    return voc, text\n",
        "voc, tex = preprocess(text, size=10000)\n",
        "print(voc[:20], tex)"
      ],
      "metadata": {
        "id": "DEeivdkeGk5O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3b4a505-5db6-4468-f363-9cf11b8fd31a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['', '[UNK]', 'the', 'and', 'of', 'to', 'that', 'in', 'he', 'shall', 'unto', 'for', 'i', 'his', 'a', 'lord', 'they', 'be', 'is', 'him'] tf.Tensor([b'the' b'first' b'book' ... b'you' b'all' b'amen'], shape=(791829,), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Option 2: Normalize text, split words and sort from most common down\n",
        "def preprocess_voc(text,size):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"[^a-z]+\", \" \", text)\n",
        "    text = tf.strings.split(text) \n",
        "    text_n = text.numpy().tolist()\n",
        "    text_n.sort()\n",
        "    counts = { text_n[0] : 1 }\n",
        "    current_word = text_n[0]\n",
        "    for i in text_n[1:]: \n",
        "        if i == current_word:\n",
        "            counts[current_word] += 1\n",
        "        else:\n",
        "            current_word = i\n",
        "            counts.update({current_word:1})\n",
        "    counts = {key: val for key, val in sorted(counts.items(), key = lambda ele: ele[1], reverse = True)}\n",
        "    voc = tf.convert_to_tensor(list(counts.keys())[:size])\n",
        "    return text, voc\n",
        "\n",
        "tex, voc = preprocess_voc(text,size=10000)\n",
        "print(voc, tex[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cogh0cJfferw",
        "outputId": "05a5c197-b1be-487b-9747-ac53d7c02233"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([b'the' b'and' b'of' ... b'forbiddeth' b'forborn' b'forcible'], shape=(10000,), dtype=string) tf.Tensor(\n",
            "[b'the' b'first' b'book' b'of' b'moses' b'called' b'genesis' b'in' b'the'\n",
            " b'beginning'], shape=(10,), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def words_to_number(text, voc):\n",
        "    text = text.numpy().tolist()\n",
        "    voc = voc.numpy().tolist()\n",
        "    for item in range(len(text)):\n",
        "        if text[item] in voc:\n",
        "            text[item] = voc.index(text[item])\n",
        "        else: \n",
        "            # UNK = index 10000\n",
        "            text[item] = 10000\n",
        "    return text\n",
        "\n",
        "tex_n = words_to_number(tex,voc)\n",
        "print(len(tex_n), tex_n[:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IuYh9W_Zwv-h",
        "outputId": "9c48539d-5fcc-4318-92e3-a8aaeb74b631"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "791829 [0, 216, 401, 2, 132, 160, 10000, 5, 0, 680, 26, 1297, 0, 170, 1, 0, 111, 1, 0, 111]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Input-Target-Pairs"
      ],
      "metadata": {
        "id": "6yn03T8L3rLC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def input_target_pairs(text):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(text)\n",
        "\n",
        "    iterator = iter(dataset) \n",
        "    iterator.get_next()\n",
        "    shift1 = dataset.map(lambda x: iterator.get_next())\n",
        "\n",
        "    iterator2 = iter(dataset) \n",
        "    iterator2.get_next()\n",
        "    iterator2.get_next()\n",
        "    shift2 = dataset.map(lambda x: iterator2.get_next())\n",
        "\n",
        "    shift1up = tf.data.Dataset.zip((dataset, shift1))\n",
        "    shift2up = tf.data.Dataset.zip((dataset, shift2))\n",
        "    shift1down = tf.data.Dataset.zip((shift1, dataset))\n",
        "    shift2down = tf.data.Dataset.zip((shift2, dataset))\n",
        "\n",
        "    dataset = shift2down.concatenate(shift1up).concatenate(shift2up).concatenate(shift1down)\n",
        "    dataset = dataset.shuffle(10000).batch(64).prefetch(tf.data.AUTOTUNE)\n",
        "    return dataset\n",
        "\n",
        "\n",
        "train_ds = input_target_pairs(tex_n[:math.ceil(len(tex_n)*0.75)])\n",
        "test_ds = input_target_pairs(tex_n[math.ceil(len(tex_n)*0.75):])\n",
        "for x,t in train_ds.take(1):\n",
        "    tf.print(x.shape, t.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nqI1X-LS3u2u",
        "outputId": "b9b510f8-9a13-45d7-8e4a-2d928017d29d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorShape([64]) TensorShape([64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Class"
      ],
      "metadata": {
        "id": "aUQnJDx7_SkO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SkipGram(tf.keras.layers.Layer):\n",
        "    def __init__(self, voc_size, emb_size):\n",
        "        super().__init__()\n",
        "    \n",
        "        self.voc_size = voc_size\n",
        "        self.emb_size = emb_size\n",
        "\n",
        "        self.opt = tf.keras.optimizers.Adam()\n",
        "\n",
        "        self.metrics_list = [tf.keras.metrics.Mean(name=\"loss\")]\n",
        "    \n",
        "    def build(self):\n",
        "        self.score = self.add_weight(shape=(self.voc_size, self.emb_size),\n",
        "                                     initializer='random_normal',\n",
        "                                     trainable=True)\n",
        "        self.score_bias = self.add_weight(shape=(self.emb_size,),\n",
        "                                   initializer='random_normal',\n",
        "                                   trainable=True)\n",
        "        self.emb = self.add_weight(shape=(self.voc_size, self.emb_size),\n",
        "                                   initializer='random_normal',\n",
        "                                   trainable=True)\n",
        "\n",
        "    def __call__(self, x, training=False):\n",
        "        emb = tf.nn.embedding_lookup(self.emb, x)\n",
        "        return emb\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return self.metrics_list\n",
        "    \n",
        "    def reset_metrics(self):\n",
        "        for metric in self.metrics:\n",
        "            metric.reset_state()   \n",
        "\n",
        "    def train(self, data):\n",
        "        input, target = data\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            y = self(input, training=True)\n",
        "            loss = tf.nn.nce_loss(weights=self.score, biases=self.score_bias, labels=target, inputs=y, \n",
        "                                  num_sampled=2, num_classes=self.voc_size)\n",
        "            loss = tf.reduce_mean(loss)\n",
        "            \n",
        "        gradients = tape.gradient(loss, self.trainable_variables)\n",
        "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
        "        \n",
        "        self.metrics[0].update_state(loss)\n",
        "        \n",
        "        return {m.name : m.result() for m in self.metrics} \n",
        "\n",
        "    def test(self,data): \n",
        "        input, target = data\n",
        "\n",
        "        y = self(input, training=False)\n",
        "        loss = tf.nn.nce_loss(weights=self.score, labels=target, inputs=y, \n",
        "                              num_sampled=2, num_classes=self.voc_size)\n",
        "        loss = tf.reduce_mean(loss)\n",
        "        \n",
        "        self.metrics[0].update_state(loss)\n",
        "        \n",
        "        return {m.name : m.result() for m in self.metrics}   "
      ],
      "metadata": {
        "id": "UKEGJ_hn_Thx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_skip = SkipGram(10001, 64)\n",
        "model_skip.build()"
      ],
      "metadata": {
        "id": "SY7RuVZNOdOu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tqdm\n",
        "\n",
        "def training_loop(model, train, val, epochs):\n",
        "    \"\"\"Train and test the SkipGram for given epochs on given data\"\"\"\n",
        "\n",
        "    # Save loss in a list for visualization\n",
        "    lists = []\n",
        "\n",
        "    for n in range(epochs):\n",
        "        print(f\"Epoch {n}:\")\n",
        "        \n",
        "        for data in tqdm.tqdm(train, position=0, leave=True):\n",
        "            metrics = model.train(data)\n",
        "\n",
        "        # Add metrics to list\n",
        "        lists.append(metrics)\n",
        "        print([f\"{key}: {value.numpy()}\" for (key,value) in metrics.items()])\n",
        "        model.reset_metrics()\n",
        "\n",
        "        for data in tqdm.tqdm(val, position=0, leave=True):\n",
        "            metrics = model.test(data)\n",
        "\n",
        "        # Add metrics to list\n",
        "        lists.append(metrics)\n",
        "        print([f\"{key}: {value.numpy()}\" for (key,value) in metrics.items()])\n",
        "        model.reset_metrics()\n",
        "        \n",
        "    return lists\n",
        "\n",
        "li = training_loop(model_skip, train_ds, test_ds, epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 783
        },
        "id": "xT-AmUgwO6o2",
        "outputId": "aa2b4e45-95c7-4f71-f5e3-e928bbb294bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/2375488 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "() ()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-53051216a3d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mli\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_skip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-19-53051216a3d5>\u001b[0m in \u001b[0;36mtraining_loop\u001b[0;34m(model, train, val, epochs)\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m# Add metrics to list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-f59113c1ec90>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             loss = tf.nn.nce_loss(weights=self.score, biases=self.score_bias, labels=target, inputs=y, \n\u001b[0m\u001b[1;32m     43\u001b[0m                                   num_sampled=2, num_classes=self.voc_size)\n\u001b[1;32m     44\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   7162\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7163\u001b[0m   \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7164\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: true_classes must be a matrix [Op:LogUniformCandidateSampler]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7og-UjKyHzXP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}