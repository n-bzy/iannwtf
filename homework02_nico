# -*- coding: utf-8 -*-
"""homework02.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QQ-8jqDCLPIWSyJb5B5inVqaZKQxc66z
"""

import numpy as np
import matplotlib as mpl
import matplotlib.pyplot as plt
#import tensorflow as tf

class Layer:
  """Creates a layer of n perceptrons"""
  def __init__(self, n_units, input_units):
    """Constructor sets up the weights (random values), bias (zeros), 
    layer input, layer preactivation and layer activation"""
    self.bias = np.zeros((1,n_units))
    self.weights = np.random.default_rng().random((input_units,n_units))*2-1
    self.layer_input = None
    self.layer_preactivation = None
    self.layer_activation = None

  def forward_step(self, input):
    """Return each units activation using ReLu as activation function"""
    input = np.array(input)
    self.layer_input = np.append(input,[1]) # Add [1] at the end for calc of bias
    # link weights and bias in one matrix
    w_b = np.append(self.weights,self.bias,axis=0) 
    self.layer_preactivation = self.layer_input@w_b
    output = np.maximum(0,self.layer_preactivation) # ReLu activation function
    self.layer_activation = output
    return output
  
  def backward_step(self, l_a,gamma = 0.03):
    """Updates each units parameters (weigths and bias)"""
    # Derivative of ReLu = 1
    # Gradient w.r.t layers weights
    self.layer_input = np.array([self.layer_input])
    grad_l_w = self.layer_input.transpose()@(np.maximum(0,1)*l_a.transpose())    # old: self.layer_input.reshape(self.layer_input.size,1)                  
    print(grad_l_w.shape)
    # Gradient w.r.t layers bias
    grad_l_b = (np.maximum(0,1)*l_a)                      
    # Gradient w.r.t layers input
    grad_l_i = (np.maximum(0,1)*l_a)@self.weights.transpose()                                         
    # Updating of layers parameters with learning rate gamma
    self.weights = self.weights - gamma*grad_l_w
    self.bias = self.bias - gamma*grad_l_b
    self.layer_input = self.layer_input - gamma*grad_l_i

class MLP:
  """Creates a Multi-Layer-Perceptron which combines instances of class Layer"""
  def __init__(self, n_layers, n_units, input_units, input):
    self.n_layers = n_layers
    self.n_units = n_units
    self.input = np.array(input)
    # Create a list of all layers with respective n units
    self.layers = []
    for n in range(n_layers):
      self.layers.append(Layer(n_units[n], input_units[n]))

  def forward_step(self):
    output = np.empty(self.input.shape) # Only works for output layer with 1 unit
    for n in range(self.n_layers):
      z = np.size(self.input,0)
      y = np.empty((z,self.n_units[n]))
      for k in range(z):
        y[k,:] = self.layers[n].forward_step(self.input[k,:])
      # Last layer = output
      if n == self.n_layers:
        output[k,:] = y
      # Hidden layer = input for next layer
      else:
        self.input = y
    return output

  def backpropagation(self, l_d):
    # How to get correct input for each layer? self.layers[1].layer_input.shape
    # Does not work atm
    self.layers[1].backward_step(l_d)

x = np.random.rand(100,1)
t = x**3-x**2+1
#plt.plot(x, t, '.')
#plt.show()

#test = Layer(4,x.size)
#test.forward_step(x)
test = MLP(2, [10,1], [1, 10], x)
output = test.forward_step()
loss = np.mean(1/2*(output - t)**2)
l_d = output - t
print(l_d.shape)
test.backpropagation(l_d)
