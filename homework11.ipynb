{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMVJXAnZcihuDNxqBzZi4s9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/n-bzy/iannwtf/blob/main/homework11.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow_text\n",
        "!pip install sentencepiece"
      ],
      "metadata": {
        "id": "-_bddBi8C4oG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWbuthdDOcFr",
        "outputId": "e4140979-c907-46af-8981-2db592aa71ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "The First Book of Moses:  Called Genesis\n",
            "\n",
            "\n",
            "1:1 In the beginning God created the heaven and the earth\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_text as tf_txt\n",
        "import sentencepiece as sp\n",
        "import numpy as np\n",
        "import re\n",
        "import io\n",
        "import datetime\n",
        "import tqdm\n",
        "import math\n",
        "\n",
        "# bash code to mount the drive\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "os.chdir(\"/content/drive/MyDrive\")\n",
        "\n",
        "file_path = f\"/content/drive/MyDrive/bible.txt\"\n",
        "\n",
        "with open(file_path, \"r\") as f:\n",
        "    text = f.read()\n",
        "\n",
        "print(text[:100])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(text, voc_size):\n",
        "    \"\"\"Preprocess text data by lower case, remove special characters and split \n",
        "    words with a SentencePie tokenizer\n",
        "    input: text,vocabulary size\n",
        "    output: tokenized text\"\"\"\n",
        "\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"[^a-z]+\", \" \", text)\n",
        "\n",
        "    sp.SentencePieceTrainer.train(\n",
        "    input=f\"/content/drive/MyDrive/bible.txt\", model_prefix='tokenizer_model', model_type=\"unigram\", vocab_size=voc_size)\n",
        "\n",
        "    # deserialize the trained model file to load it in the correct format\n",
        "    trained_tokenizer_model = tf.io.gfile.GFile('tokenizer_model.model', \"rb\").read()\n",
        "\n",
        "    # load the model as a tokenizer that can be used inside a tensorflow model\n",
        "    tokenizer = tf_txt.SentencepieceTokenizer(\n",
        "        model=trained_tokenizer_model, out_type=tf.int32, nbest_size=-1, alpha=1, reverse=False,\n",
        "        add_bos=False, add_eos=False, return_nbest=False, name=None)\n",
        "    \n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    return tokens, tokenizer \n",
        "\n",
        "tokens, tokenizer = tokenize(text, voc_size=5000)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SRERog0aV2aG",
        "outputId": "5b1611ae-0bf1-4a5a-f5de-84aa6268bd2c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([   4  273  562 ...   31   19 1670], shape=(949085,), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def input_target(tokens,seq_length,batch_size):\n",
        "    #data = tf.data.Dataset.from_tensor_slices(tokens)\n",
        "    data = tf_txt.sliding_window(tokens, width=seq_length+1)\n",
        "\n",
        "    #input = data[:seq_length]\n",
        "    #target = data[1:]\n",
        "\n",
        "    #data = tf.data.Dataset.zip((tf.data.Dataset.from_tensor_slices(input), tf.data.Dataset.from_tensor_slices(target)))\n",
        "    data = tf.data.Dataset.from_tensor_slices(data)\n",
        "    data = data.shuffle(1024).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "    return data\n",
        "\n",
        "#ds = input_target(tokens, seq_length=128, batch_size=64)\n",
        "train_ds = input_target(tokens[:math.ceil(len(tokens)*0.9)], seq_length=128, batch_size=128)\n",
        "test_ds = input_target(tokens[math.ceil(len(tokens)*0.9):], seq_length=128, batch_size=128)\n",
        "\n",
        "for x in train_ds.take(1):\n",
        "    print(x.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y_WbAWesbkEq",
        "outputId": "0856e46b-1f82-4c08-d104-eeef167fef79"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(128, 129)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Emb(tf.keras.layers.Layer):\n",
        "    \"\"\"\"\"\"\n",
        "    def __init__(self, voc_size, emb_size, seq_length):\n",
        "        super().__init__()\n",
        "\n",
        "        self.emb = tf.keras.layers.Embedding(input_dim=voc_size,output_dim=emb_size)\n",
        "        self.pos = tf.keras.layers.Embedding(input_dim=seq_length,output_dim=emb_size)\n",
        "\n",
        "    def call(self,input):\n",
        "        t = tf.range(0,len(input))\n",
        "        t = tf.expand_dims(t,-1)\n",
        "        x = self.emb(input)\n",
        "        y = self.pos(t)\n",
        "        #print(x.shape, y.shape)\n",
        "        z = x+y\n",
        "        return z\n"
      ],
      "metadata": {
        "id": "lpEuxh1ifbWn"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(tf.keras.layers.Layer):\n",
        "    \"\"\"\"\"\"\n",
        "    def __init__(self, emb_size):\n",
        "        super().__init__()\n",
        "\n",
        "        self.mha = tf.keras.layers.MultiHeadAttention(num_heads=3,key_dim=emb_size)\n",
        "        self.denseRELU = tf.keras.layers.Dense(128,activation='relu')\n",
        "        self.dense = tf.keras.layers.Dense(emb_size)\n",
        "        self.drop1 = tf.keras.layers.Dropout(0.1)\n",
        "        self.drop2 = tf.keras.layers.Dropout(0.1)\n",
        "        self.ln1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.ln2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    def call(self, input, training=False):\n",
        "        x = self.mha(query=input,value=input, use_causal_mask=True)\n",
        "        x = self.drop1(x, training=training)\n",
        "        x = x + input\n",
        "        x = self.ln1(x)\n",
        "        z = self.denseRELU(x)\n",
        "        z = self.dense(z)\n",
        "        z = self.drop2(z, training=training)\n",
        "        y = x + z\n",
        "        y = self.ln2(y)\n",
        "        return y\n"
      ],
      "metadata": {
        "id": "_hJNoTZmG0Jl"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "    \"\"\"\"\"\"\n",
        "    def __init__(self, voc_size, emb_size, seq_length, tokenizer):\n",
        "        super().__init__()\n",
        "\n",
        "        self.opt = tf.keras.optimizers.Adam()\n",
        "        self.loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "        self.metrics_list = [tf.keras.metrics.Mean(name=\"loss\")]\n",
        "        \n",
        "        self.emb = Emb(voc_size, emb_size, seq_length)\n",
        "        self.tfb = TransformerBlock(emb_size)\n",
        "        self.dense = tf.keras.layers.Dense(voc_size)\n",
        "        \n",
        "        self.tokenizer = tokenizer\n",
        "        self.seq_length = seq_length\n",
        "\n",
        "    def call(self, input,  training=False):\n",
        "        x = self.emb(input)\n",
        "        x = self.tfb(x, training=training)\n",
        "        x = self.dense(x)\n",
        "        return x\n",
        "\n",
        "    def reset_metrics(self):\n",
        "        for metric in self.metrics:\n",
        "            metric.reset_states()\n",
        "            \n",
        "    @tf.function\n",
        "    def train_step(self, data):\n",
        "        \n",
        "        x = data[:,:self.seq_length]\n",
        "        t = data[:,1:]\n",
        "        \n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self(x, training=True)\n",
        "            loss = self.loss(t, predictions) + tf.reduce_sum(self.losses)\n",
        "        \n",
        "        gradients = tape.gradient(loss, self.trainable_variables)\n",
        "        self.opt.apply_gradients(zip(gradients, self.trainable_variables))\n",
        "        \n",
        "        # update loss metric\n",
        "        self.metrics[0].update_state(loss)\n",
        "\n",
        "        # Return a dictionary mapping metric names to current value\n",
        "        return {m.name: m.result() for m in self.metrics}\n",
        "\n",
        "    @tf.function\n",
        "    def test_step(self, data):\n",
        "\n",
        "        x = data[:,:self.seq_length]\n",
        "        t = data[:,1:]\n",
        "\n",
        "        predictions = self(x, training=False)\n",
        "        loss = self.loss(t, predictions) + tf.reduce_sum(self.losses)\n",
        "\n",
        "        self.metrics[0].update_state(loss)\n",
        "\n",
        "        return {m.name: m.result() for m in self.metrics}\n",
        "    \n",
        "    def generate_text(self, prompt, output_length, top_k):\n",
        "        tokens = self.tokenizer.tokenize(prompt)\n",
        "        for _ in range(output_length):\n",
        "            #x = tf.expand_dims(tokens, 0)\n",
        "\n",
        "            y = self(x, training=False)\n",
        "            highest_logits = tf.math.top_k(y, k = top_k, sorted = True)\n",
        "            sample_number = tf.random.uniform(shape=(), minval=0, maxval=top_k, dtype=tf.int32)\n",
        "            vocabulary_index = highest_logits.indices.numpy()[0, -1, sample_number]\n",
        "            tokens = tf.concat([tokens, [vocabulary_index]], -1)\n",
        "        \n",
        "        return self.tokenizer.detokenize(tokens)"
      ],
      "metadata": {
        "id": "0e17pYaCKDRQ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Transformer(voc_size=5000, emb_size=128, seq_length=128, tokenizer=tokenizer)\n",
        "\n",
        "# run model on input once so the layers are built\n",
        "model(tf.keras.Input((129)));\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2a67r2SdbL8M",
        "outputId": "343d47db-c4ea-425f-c09a-b3d3f9b83d43"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"transformer\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " emb (Emb)                   multiple                  656384    \n",
            "                                                                 \n",
            " transformer_block (Transfor  multiple                 231424    \n",
            " merBlock)                                                       \n",
            "                                                                 \n",
            " dense_2 (Dense)             multiple                  645000    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,532,810\n",
            "Trainable params: 1,532,808\n",
            "Non-trainable params: 2\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tqdm\n",
        "\n",
        "def training_loop(model, train_ds, val_ds, epochs):#, train_summary_writer, val_summary_writer):\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"Epoch {epoch}:\")\n",
        "        \n",
        "        for data in tqdm.tqdm(train_ds, position=0, leave=True):\n",
        "            metrics = model.train_step(data)\n",
        "            \n",
        "            # logging the validation metrics to the log file which is used by tensorboard\n",
        "            #with train_summary_writer.as_default():\n",
        "                #for metric in model.metrics:\n",
        "                    #tf.summary.scalar(f\"{metric.name}\", metric.result(), step=epoch)\n",
        "\n",
        "        # print the metrics\n",
        "        print([f\"{key}: {value.numpy()}\" for (key, value) in metrics.items()])\n",
        "\n",
        "        # reset all metrics (requires a reset_metrics method in the model)\n",
        "        model.reset_metrics()    \n",
        "        \n",
        "        # Validation:\n",
        "        for data in val_ds:\n",
        "            metrics = model.test_step(data)\n",
        "        \n",
        "            # logging the validation metrics to the log file which is used by tensorboard\n",
        "            #with val_summary_writer.as_default():\n",
        "                #for metric in model.metrics:\n",
        "                    #tf.summary.scalar(f\"{metric.name}\", metric.result(), step=epoch)\n",
        "                    \n",
        "        print([f\"val_{key}: {value.numpy()}\" for (key, value) in metrics.items()])\n",
        "\n",
        "        # reset all metrics\n",
        "        model.reset_metrics()\n",
        "        #print(\"\\n\")\n",
        "\n",
        "        gen_text = model.generate_text('What is', 5, 5)\n",
        "        tf.print(gen_text)\n",
        "\n",
        "training_loop(model,train_ds, test_ds, epochs=10,)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R0I5ztfgb-UX",
        "outputId": "67c19634-0287-4040-9d4f-ad5829b7e6dd"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6673/6673 [06:58<00:00, 15.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['loss: 3.542757987976074']\n",
            "['val_loss: 5.03262186050415']\n",
            "tf.Tensor(b'What is given ordained seen been ordained', shape=(), dtype=string)\n",
            "Epoch 1:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6673/6673 [06:54<00:00, 16.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['loss: 3.465855360031128']\n",
            "['val_loss: 5.095674991607666']\n",
            "tf.Tensor(b'What is ordained been done been ordained', shape=(), dtype=string)\n",
            "Epoch 2:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6673/6673 [06:54<00:00, 16.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['loss: 3.4033737182617188']\n",
            "['val_loss: 5.146303176879883']\n",
            "tf.Tensor(b'What is made done been done ordained', shape=(), dtype=string)\n",
            "Epoch 3:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6673/6673 [06:52<00:00, 16.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['loss: 3.3606479167938232']\n",
            "['val_loss: 5.211519241333008']\n",
            "tf.Tensor(b'What is made spoken been been made', shape=(), dtype=string)\n",
            "Epoch 4:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6673/6673 [06:52<00:00, 16.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['loss: 3.33132266998291']\n",
            "['val_loss: 5.261597633361816']\n",
            "tf.Tensor(b'What is made seen made ordained made', shape=(), dtype=string)\n",
            "Epoch 5:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6673/6673 [06:51<00:00, 16.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['loss: 3.3094565868377686']\n",
            "['val_loss: 5.309690952301025']\n",
            "tf.Tensor(b'What is been ordained made ordained been', shape=(), dtype=string)\n",
            "Epoch 6:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6673/6673 [06:51<00:00, 16.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['loss: 3.292238473892212']\n",
            "['val_loss: 5.371089458465576']\n",
            "tf.Tensor(b'What is ordained been said made ordained', shape=(), dtype=string)\n",
            "Epoch 7:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6673/6673 [06:51<00:00, 16.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['loss: 3.2793240547180176']\n",
            "['val_loss: 5.397314548492432']\n",
            "tf.Tensor(b'What is been made made made made', shape=(), dtype=string)\n",
            "Epoch 8:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6673/6673 [06:51<00:00, 16.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['loss: 3.269660711288452']\n",
            "['val_loss: 5.410665035247803']\n",
            "tf.Tensor(b'What is ordained done been made been', shape=(), dtype=string)\n",
            "Epoch 9:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6673/6673 [07:21<00:00, 15.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['loss: 3.2612290382385254']\n",
            "['val_loss: 5.424385070800781']\n",
            "tf.Tensor(b'What is raised been made said made', shape=(), dtype=string)\n"
          ]
        }
      ]
    }
  ]
}